{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "01beebdb",
      "metadata": {
        "id": "01beebdb"
      },
      "source": [
        "# Fake News Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df0944b7",
      "metadata": {
        "id": "df0944b7"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "125eae54",
      "metadata": {
        "id": "125eae54"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    \"\"\"Return (accuracy, precision, recall, f1).\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Use 'binary' for binary classification, otherwise fallback to macro.\n",
        "    avg = \"binary\" if len(np.unique(y_true)) == 2 else \"macro\"\n",
        "\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=avg, zero_division=0\n",
        "    )\n",
        "    return acc, prec, rec, f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c83ee37a",
      "metadata": {
        "id": "c83ee37a"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Put the dataset file in the same folder as this notebook (recommended), or provide an absolute path.\n",
        "\n",
        "This dataset uses **semicolon-separated** fields and can contain extra semicolons inside the text.\n",
        "So we use a custom loader that safely reconstructs the text column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ce5700b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce5700b1",
        "outputId": "8a18c111-a363-4d3b-915f-d13eea6c3173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: (7815, 4)\n",
            "Label distribution:\n",
            " label\n",
            "1    4185\n",
            "0    3630\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# === Data path ===\n",
        "# If your file has a different name/path, update it here.\n",
        "DATA_PATH = \"evaluation.csv\"   # <-- change if needed\n",
        "\n",
        "def load_semicolon_dataset(path):\n",
        "    \"\"\"\n",
        "    Handles lines like:\n",
        "    ;title;text;label\n",
        "    0;some title;some text that may contain ; ; ; ;0\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        _ = f.readline()  # header\n",
        "        for line in f:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(\";\")\n",
        "            if len(parts) < 4:\n",
        "                continue\n",
        "\n",
        "            idx = parts[0]\n",
        "            title = parts[1]\n",
        "            label = parts[-1]\n",
        "            text = \";\".join(parts[2:-1])  # re-join any extra ';' inside text\n",
        "            rows.append((idx, title, text, label))\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"id\", \"title\", \"text\", \"label\"])\n",
        "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
        "    df[\"label\"] = df[\"label\"].astype(int)\n",
        "    return df\n",
        "\n",
        "df = load_semicolon_dataset(DATA_PATH)\n",
        "print(\"Dataset:\", df.shape)\n",
        "print(\"Label distribution:\\n\", df[\"label\"].value_counts())\n",
        "\n",
        "# Combine title + text into one string per document\n",
        "docs = (df[\"title\"].fillna(\"\") + \" \" + df[\"text\"].fillna(\"\")).astype(str).tolist()\n",
        "y = df[\"label\"].values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6df6766",
      "metadata": {
        "id": "b6df6766"
      },
      "source": [
        "## Train/test split\n",
        "\n",
        "We keep a standard **80/20** split with stratification (preserves label ratio).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d3ca0b1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3ca0b1a",
        "outputId": "14bb8e2a-4e05-4cff-9ffc-f46a489c2ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train docs: 6252 Test docs: 1563\n"
          ]
        }
      ],
      "source": [
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    docs, y,\n",
        "    test_size=0.20,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train docs:\", len(X_train_text), \"Test docs:\", len(X_test_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39543025",
      "metadata": {
        "id": "39543025"
      },
      "source": [
        "## Case Study: Bag-of-Words Features (CountVectorizer)\n",
        "\n",
        "We need to convert text into numeric features before we can train ML models.\n",
        "\n",
        "**CountVectorizer** builds a vocabulary from the **training set** and represents each document as a vector of **counts** (one entry per vocabulary term).\n",
        "\n",
        "We will use:\n",
        "$$\n",
        "\\texttt{CountVectorizer(}\n",
        "\\texttt{lowercase=True, stop_words=\"english\", ngram_range=(1,2),}\n",
        "$$\n",
        "$$\n",
        "\\texttt{ min_df=2, max_df=0.9, max_features=10000)}\n",
        "$$\n",
        "\n",
        "**What each setting means (briefly):**\n",
        "- `lowercase=True`: convert text to lowercase before building features.\n",
        "- `stop_words=\"english\"`: remove a predefined list of common English words.\n",
        "- `ngram_range=(1,2)`: allow 1-word features and 2-word features (bigrams).\n",
        "- `min_df=2`: keep a term only if it appears in at least 2 training documents.\n",
        "- `max_df=0.9`: drop a term if it appears in more than 90% of training documents.\n",
        "- `max_features=10000`: cap the vocabulary size at 10,000 terms (after filtering).\n",
        "\n",
        "### Tiny example (just to see what it does)\n",
        "\n",
        "We will build features from 3 short documents and look at the counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaee0c65",
      "metadata": {
        "id": "aaee0c65"
      },
      "outputs": [],
      "source": [
        "# CountVectorizer docs (read this once before TODO 1):\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b5a2cb96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5a2cb96",
        "outputId": "d16b51bc-2125-4a02-d5c0-5949a9b56e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toy vocab: ['fake', 'fake news', 'news', 'news spreads', 'spreads']\n",
            "Toy counts (rows = docs):\n",
            " [[1 1 1 1 1]\n",
            " [1 1 1 1 1]\n",
            " [0 0 1 1 1]]\n"
          ]
        }
      ],
      "source": [
        "toy_docs = [\n",
        "    \"The FAKE news spreads fast\",\n",
        "    \"Fake news spreads\",\n",
        "    \"Real news spreads\",\n",
        "]\n",
        "\n",
        "toy_vec = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2\n",
        ")\n",
        "\n",
        "toy_X = toy_vec.fit_transform(toy_docs)\n",
        "\n",
        "print(\"Toy vocab:\", list(toy_vec.get_feature_names_out()))\n",
        "print(\"Toy counts (rows = docs):\\n\", toy_X.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "081e3f66",
      "metadata": {
        "id": "081e3f66"
      },
      "source": [
        "## Build Bag-of-Words features\n",
        "\n",
        "Goal:\n",
        "1. Create the `CountVectorizer` using the exact settings below.\n",
        "2. Fit on the training text only.\n",
        "3. Transform both train and test text into sparse Bag-of-Words features.\n",
        "\n",
        "Notes:\n",
        "- `fit_transform` on train, then `transform` on test.\n",
        "- The output is a **sparse matrix** (CSR). That is normal for text features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a6a1edcc",
      "metadata": {
        "id": "a6a1edcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73e8b2c-e178-45d4-8438-4fb553b0b766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW shapes: (6252, 10000) (1563, 10000)\n"
          ]
        }
      ],
      "source": [
        "# --- Bag-of-Words settings ---\n",
        "MAX_FEATURES = 10000\n",
        "NGRAM_RANGE = (1, 2)\n",
        "\n",
        "# 1) Create `vectorizer` using CountVectorizer with:\n",
        "#    lowercase=True\n",
        "#    stop_words=\"english\"\n",
        "#    ngram_range=NGRAM_RANGE\n",
        "#    min_df=2\n",
        "#    max_df=0.9\n",
        "#    max_features=MAX_FEATURES\n",
        "#\n",
        "# 2) Fit the vectorizer on the training text, then use it to transform:\n",
        "#    - the training text into BoW features\n",
        "#    - the test text into BoW features\n",
        "#\n",
        "# Print the BoW shapes.\n",
        "vectorizer = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=NGRAM_RANGE,\n",
        "    min_df=2,\n",
        "    max_df=0.9,\n",
        "    max_features=MAX_FEATURES\n",
        ")\n",
        "\n",
        "X_train_bow = vectorizer.fit_transform(X_train_text)\n",
        "X_test_bow = vectorizer.transform(X_test_text)\n",
        "\n",
        "print(\"BoW shapes:\", X_train_bow.shape, X_test_bow.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5127e270",
      "metadata": {
        "id": "5127e270"
      },
      "source": [
        "## Models\n",
        "\n",
        "Create **7 classifiers** using the exact hyperparameters below.\n",
        "\n",
        "**Important:** For KNN in this notebook, start with **Euclidean distance**.\n",
        "\n",
        "Models to implement:\n",
        "- Logistic Regression: `solver=\"saga\"`, `max_iter=2000`, `n_jobs=-1`, `random_state=42`\n",
        "- Perceptron: `max_iter=1000`, `tol=1e-3`, `random_state=42`\n",
        "- SVM (LinearSVC): `random_state=42`\n",
        "- Naive Bayes (MultinomialNB): `alpha=1.0`\n",
        "- KNN (Euclidean): `n_neighbors=7`, `metric=\"euclidean\"`, `n_jobs=-1`\n",
        "- Decision Tree: `max_depth=40`, `random_state=42`\n",
        "- Random Forest: `n_estimators=300`, `random_state=42`, `n_jobs=-1`\n",
        "\n",
        "Put them in a dictionary named `models`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "78772066",
      "metadata": {
        "id": "78772066",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3dc0dda-30f5-46ed-fc91-38776ea867d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Logistic Regression': LogisticRegression(max_iter=2000, n_jobs=-1, random_state=42, solver='saga'),\n",
              " 'Perceptron': Perceptron(random_state=42),\n",
              " 'SVM (LinearSVC)': LinearSVC(random_state=42),\n",
              " 'Naive Bayes (MultinomialNB)': MultinomialNB(),\n",
              " 'KNN (euclidean)': KNeighborsClassifier(metric='euclidean', n_jobs=-1, n_neighbors=7),\n",
              " 'Decision Tree': DecisionTreeClassifier(max_depth=40, random_state=42),\n",
              " 'Random Forest': RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=42)}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Build the `models` dictionary using the exact parameters above.\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(solver=\"saga\", max_iter=2000, n_jobs=-1, random_state=42),\n",
        "    \"Perceptron\": Perceptron(max_iter=1000, tol=1e-3, random_state=42),\n",
        "    \"SVM (LinearSVC)\": LinearSVC(random_state=42),\n",
        "    \"Naive Bayes (MultinomialNB)\": MultinomialNB(alpha=1.0),\n",
        "    \"KNN (euclidean)\": KNeighborsClassifier(n_neighbors=7, metric=\"euclidean\", n_jobs=-1),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(max_depth=40, random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
        "}\n",
        "\n",
        "models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1adacab",
      "metadata": {
        "id": "c1adacab"
      },
      "source": [
        "## Train + evaluate\n",
        "\n",
        "We will evaluate each model on:\n",
        "- **Training set**\n",
        "- **Test set**\n",
        "\n",
        "Metrics:\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1\n",
        "\n",
        "We will print a table sorted by **Test F1**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "10fdf1af",
      "metadata": {
        "id": "10fdf1af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a544e6-bb52-44bb-9b3e-81929553ad07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Results (sorted by Test F1) ===\n",
            "                      Model Train Acc Train Prec Train Rec Train F1 Test Acc Test Prec Test Rec Test F1\n",
            "              Random Forest    1.0000     1.0000    1.0000   1.0000   0.9942    0.9964   0.9928  0.9946\n",
            "              Decision Tree    1.0000     1.0000    1.0000   1.0000   0.9904    0.9952   0.9869  0.9910\n",
            "                 Perceptron    1.0000     1.0000    1.0000   1.0000   0.9878    0.9869   0.9904  0.9887\n",
            "        Logistic Regression    0.9989     0.9997    0.9982   0.9990   0.9872    0.9869   0.9892  0.9881\n",
            "            SVM (LinearSVC)    1.0000     1.0000    1.0000   1.0000   0.9853    0.9857   0.9869  0.9863\n",
            "Naive Bayes (MultinomialNB)    0.9624     0.9684    0.9612   0.9648   0.9616    0.9642   0.9642  0.9642\n",
            "            KNN (euclidean)    0.7826     0.8905    0.6774   0.7695   0.7351    0.8555   0.6081  0.7109\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "  # 1) fits each model on X_train_bow, y_train\n",
        "  model.fit(X_train_bow, y_train)\n",
        "\n",
        "  # 2) predicts on train and test\n",
        "  yhat_train = model.predict(X_train_bow)\n",
        "  yhat_test = model.predict(X_test_bow)\n",
        "\n",
        "  # 3) computes (acc, prec, rec, f1) using metrics(...)\n",
        "  train_acc, train_prec, train_rec, train_f1 = metrics(y_train, yhat_train)\n",
        "  test_acc, test_prec, test_rec, test_f1 = metrics(y_test,  yhat_test)\n",
        "\n",
        "  # 4) stores results in a list\n",
        "  results.append([name, train_acc, train_prec, train_rec, train_f1,\n",
        "                  test_acc, test_prec, test_rec, test_f1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cols = [\n",
        "    \"Model\",\n",
        "    \"Train Acc\", \"Train Prec\", \"Train Rec\", \"Train F1\",\n",
        "    \"Test Acc\", \"Test Prec\", \"Test Rec\", \"Test F1\",\n",
        "]\n",
        "\n",
        "  # 5) prints a DataFrame sorted by Test F1 (descending)\n",
        "out = pd.DataFrame(results, columns=cols).sort_values(\"Test F1\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 80)\n",
        "print(\"\\n=== Results (sorted by Test F1) ===\")\n",
        "print(out.to_string(index=False, formatters={\n",
        "    \"Train Acc\": \"{:.4f}\".format,\n",
        "    \"Train Prec\": \"{:.4f}\".format,\n",
        "    \"Train Rec\": \"{:.4f}\".format,\n",
        "    \"Train F1\": \"{:.4f}\".format,\n",
        "    \"Test Acc\": \"{:.4f}\".format,\n",
        "    \"Test Prec\": \"{:.4f}\".format,\n",
        "    \"Test Rec\": \"{:.4f}\".format,\n",
        "    \"Test F1\": \"{:.4f}\".format,\n",
        "}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51e27447",
      "metadata": {
        "id": "51e27447"
      },
      "source": [
        "## Cosine distance for KNN\n",
        "\n",
        "With Bag-of-Words, each document becomes a long vector of word counts (mostly zeros).  \n",
        "To compare two documents, we need a way to measure how “close” two vectors are.\n",
        "\n",
        "Two common choices:\n",
        "\n",
        "- **Euclidean distance**: straight-line distance between two vectors.\n",
        "- **Cosine distance**: based on the angle between two vectors (uses cosine similarity under the hood).\n",
        "\n",
        "In scikit-learn, KNN uses a **distance**. Cosine distance is:\n",
        "$$\n",
        "d_{\\text{cosine}}(x, z) \\;=\\; 1 - \\cos(x, z)\n",
        "\\;=\\; 1 - \\frac{x^\\top z}{\\|x\\|_2 \\,\\|z\\|_2}\n",
        "$$\n",
        "\n",
        "(where $\\cos(x,z)$ is cosine similarity).\n",
        "\n",
        "### Tiny numeric example (no text, just vectors)\n",
        "\n",
        "Let:\n",
        "- $x = [1, 1]$\n",
        "- $z_1 = [2, 2]$  (same direction as $x$, just “bigger”)\n",
        "- $z_2 = [2, 0]$  (different direction)\n",
        "\n",
        "**Euclidean distances**\n",
        "$$\n",
        "\\|x - z_1\\|_2 = \\sqrt{(1-2)^2 + (1-2)^2} = \\sqrt{2}\n",
        "$$\n",
        "$$\n",
        "\\|x - z_2\\|_2 = \\sqrt{(1-2)^2 + (1-0)^2} = \\sqrt{2}\n",
        "$$\n",
        "So Euclidean says $z_1$ and $z_2$ are equally far from $x$ here.\n",
        "\n",
        "**Cosine distances**\n",
        "$$\n",
        "\\cos(x, z_1) = \\frac{1\\cdot 2 + 1\\cdot 2}{\\sqrt{2}\\cdot \\sqrt{8}} = 1\n",
        "\\Rightarrow d_{\\text{cosine}}(x, z_1)=0\n",
        "$$\n",
        "$$\n",
        "\\cos(x, z_2) = \\frac{1\\cdot 2 + 1\\cdot 0}{\\sqrt{2}\\cdot 2} \\approx 0.707\n",
        "\\Rightarrow d_{\\text{cosine}}(x, z_2)\\approx 0.293\n",
        "$$\n",
        "So cosine says $z_1$ is closer to $x$ than $z_2$.\n",
        "\n",
        "### What you will do\n",
        "\n",
        "Keep everything the same, but change your KNN metric from `\"euclidean\"` to `\"cosine\"`, then re-run your evaluation and compare results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "04779264",
      "metadata": {
        "id": "04779264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c73fd8-70cd-40f6-e274-a63deaae1c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Results (sorted by Test F1) ===\n",
            "                      Model Train Acc Train Prec Train Rec Train F1 Test Acc Test Prec Test Rec Test F1\n",
            "              Random Forest    1.0000     1.0000    1.0000   1.0000   0.9942    0.9964   0.9928  0.9946\n",
            "              Decision Tree    1.0000     1.0000    1.0000   1.0000   0.9904    0.9952   0.9869  0.9910\n",
            "                 Perceptron    1.0000     1.0000    1.0000   1.0000   0.9878    0.9869   0.9904  0.9887\n",
            "        Logistic Regression    0.9989     0.9997    0.9982   0.9990   0.9872    0.9869   0.9892  0.9881\n",
            "            SVM (LinearSVC)    1.0000     1.0000    1.0000   1.0000   0.9853    0.9857   0.9869  0.9863\n",
            "Naive Bayes (MultinomialNB)    0.9624     0.9684    0.9612   0.9648   0.9616    0.9642   0.9642  0.9642\n",
            "               KNN (cosine)    0.9131     0.8721    0.9818   0.9237   0.8797    0.8308   0.9737  0.8966\n"
          ]
        }
      ],
      "source": [
        "# Tip: For cosine distance, brute-force search is commonly used.\n",
        "# Example (do not run until TODO 2/3 are done):\n",
        "#\n",
        "# knn_cos = KNeighborsClassifier(\n",
        "#     n_neighbors=7,\n",
        "#     metric=\"cosine\",\n",
        "#     algorithm=\"brute\",\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(solver=\"saga\", max_iter=2000, n_jobs=-1, random_state=42),\n",
        "    \"Perceptron\": Perceptron(max_iter=1000, tol=1e-3, random_state=42),\n",
        "    \"SVM (LinearSVC)\": LinearSVC(random_state=42),\n",
        "    \"Naive Bayes (MultinomialNB)\": MultinomialNB(alpha=1.0),\n",
        "    \"KNN (cosine)\": KNeighborsClassifier(n_neighbors=7, metric=\"cosine\", algorithm=\"brute\", n_jobs=-1),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(max_depth=40, random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "  # 1) fits each model on X_train_bow, y_train\n",
        "  model.fit(X_train_bow, y_train)\n",
        "\n",
        "  # 2) predicts on train and test\n",
        "  yhat_train = model.predict(X_train_bow)\n",
        "  yhat_test = model.predict(X_test_bow)\n",
        "\n",
        "  # 3) computes (acc, prec, rec, f1) using metrics(...)\n",
        "  train_acc, train_prec, train_rec, train_f1 = metrics(y_train, yhat_train)\n",
        "  test_acc, test_prec, test_rec, test_f1 = metrics(y_test,  yhat_test)\n",
        "\n",
        "  # 4) stores results in a list\n",
        "  results.append([name, train_acc, train_prec, train_rec, train_f1,\n",
        "                  test_acc, test_prec, test_rec, test_f1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cols = [\n",
        "    \"Model\",\n",
        "    \"Train Acc\", \"Train Prec\", \"Train Rec\", \"Train F1\",\n",
        "    \"Test Acc\", \"Test Prec\", \"Test Rec\", \"Test F1\",\n",
        "]\n",
        "\n",
        "  # 5) prints a DataFrame sorted by Test F1 (descending)\n",
        "out = pd.DataFrame(results, columns=cols).sort_values(\"Test F1\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 80)\n",
        "print(\"\\n=== Results (sorted by Test F1) ===\")\n",
        "print(out.to_string(index=False, formatters={\n",
        "    \"Train Acc\": \"{:.4f}\".format,\n",
        "    \"Train Prec\": \"{:.4f}\".format,\n",
        "    \"Train Rec\": \"{:.4f}\".format,\n",
        "    \"Train F1\": \"{:.4f}\".format,\n",
        "    \"Test Acc\": \"{:.4f}\".format,\n",
        "    \"Test Prec\": \"{:.4f}\".format,\n",
        "    \"Test Rec\": \"{:.4f}\".format,\n",
        "    \"Test F1\": \"{:.4f}\".format,\n",
        "}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace5e4ac",
      "metadata": {
        "id": "ace5e4ac"
      },
      "source": [
        "## Discussion questions (answer in your own words)\n",
        "\n",
        "Write short answers below (2–5 sentences each is enough).\n",
        "\n",
        "### Question A\n",
        "In your own words, what is the added value of allowing 2-word sequences (bigrams) in `ngram_range`?\n",
        "\n",
        "### Question B\n",
        "In your own words, why might someone choose to set both `min_df` and `max_df` when building the vocabulary?\n",
        "\n",
        "### Question C\n",
        "\n",
        "After you run KNN with **Euclidean** and then with **Cosine** distance:\n",
        "\n",
        "- Do you observe any difference in results?\n",
        "- If yes, why do you think the difference happens (your intuition)?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d04ee569",
      "metadata": {
        "id": "d04ee569"
      },
      "source": [
        "**Your answers:**\n",
        "\n",
        "- **A:**  \n",
        "  *Bigrams brings context into the individual units of \"grams\". By having more than one word, bow now has the ability to better identify the relevancy of the individual units.*\n",
        "\n",
        "- **B:**  \n",
        "  *Manually setting both min_df and max_df gives the user more control over \"what matters and what does not\". Since user may know the dataset better, they can set those parameters to specific numbers such that more irelevant info can but pruned out and more important info is preserved.*\n",
        "\n",
        "- **C:**  \n",
        "  *KNN with cosine performed much better on the test set prediction. Looking at the testing set results, the euclidean distance KNN yielded an F-1 score of 0.7109, while the cosine distance KNN yielded an F-1 score of 0.8966.*\n",
        "\n",
        "  *I think the difference is that the cosine distance KNN helped distinguish the text-turned-vectors that are same in euclidean distance but different in directions more different that similar, yielding a better overall result.*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}